{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/scenic/core/errors.py:157: UserWarning: unable to install sys.excepthook to format Scenic backtraces\n",
      "  warnings.warn('unable to install sys.excepthook to format Scenic backtraces')\n"
     ]
    }
   ],
   "source": [
    "from scenic.simulators.gfootball import rl_interface\n",
    "from stable_baselines3 import PPO\n",
    "from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "import pretrain_template\n",
    "#from gfootball_impala_cnn import GfootballImpalaCNN\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import os\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.preprocessing import is_image_space\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from torch import nn\n",
    "import torch as th\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parliamentary-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GfootballImpalaCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    gfootball_impala_cnn is architecture used in the paper\n",
    "    (https://arxiv.org/pdf/1907.11180.pdf).\n",
    "    It is illustrated in the appendix. It is similar to Large architecture\n",
    "    from IMPALA paper; we use 4 big blocks instead of 3 though.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(GfootballImpalaCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        assert is_image_space(observation_space), (\n",
    "            \"You should use CNN only with images\"\n",
    "        )\n",
    "        assert features_dim==256, \"To replicate the same network\"\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device: \", self.device)\n",
    "\n",
    "        self.conv_layers_config = [(16, 2), (32, 2), (32, 2), (32, 2)]\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "\n",
    "        self.conv_blocks = [\n",
    "            nn.Conv2d(in_channels=n_input_channels, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        ]\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "        #if \"cuda\" in self.device.type:\n",
    "        #    self.conv_blocks = [c.cuda() for c in self.conv_blocks]\n",
    "\n",
    "        #https://www.tensorflow.org/api_docs/python/tf/nn/pool  -> If padding = \"SAME\": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\n",
    "        self.pools = [nn.MaxPool2d(kernel_size=3, stride=2, padding=1) for _ in range(4)]\n",
    "\n",
    "        self.resblocks_1 = [\n",
    "            self.create_basic_res_block(16, 16),\n",
    "            self.create_basic_res_block(32, 32),\n",
    "            self.create_basic_res_block(32, 32),\n",
    "            self.create_basic_res_block(32, 32)\n",
    "        ]\n",
    "        self.resblocks_2 = [\n",
    "            self.create_basic_res_block(16, 16),\n",
    "            self.create_basic_res_block(32, 32),\n",
    "            self.create_basic_res_block(32, 32),\n",
    "            self.create_basic_res_block(32, 32)\n",
    "        ]\n",
    "\n",
    "\n",
    "        \n",
    "        if \"cuda\" in self.device.type:\n",
    "            self.conv_blocks = [c.cuda() for c in self.conv_blocks]\n",
    "            self.resblocks_1 = [c.cuda() for c in self.resblocks_1]\n",
    "            self.resblocks_2 = [c.cuda() for c in self.resblocks_2]\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        \n",
    "        #hack so that all the weights are saved correctly, by default stable_baselines3 only saves weights all torch tensors\n",
    "        #So it\n",
    "        self.conv_0 = self.conv_blocks[0]\n",
    "        self.conv_1 = self.conv_blocks[1]\n",
    "        self.conv_2 = self.conv_blocks[2]\n",
    "        self.conv_3 = self.conv_blocks[3]\n",
    "        \n",
    "        \n",
    "        self.res_1_0 = self.resblocks_1[0]\n",
    "        self.res_1_1 = self.resblocks_1[1]\n",
    "        self.res_1_2 = self.resblocks_1[2]\n",
    "        self.res_1_3 = self.resblocks_1[3]\n",
    "\n",
    "        self.res_2_0 = self.resblocks_2[0]\n",
    "        self.res_2_1 = self.resblocks_2[1]\n",
    "        self.res_2_2 = self.resblocks_2[2]\n",
    "        self.res_2_3 = self.resblocks_2[3]\n",
    "        #################################################\n",
    "\n",
    "\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        \"\"\"\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.feat_extract(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            )\n",
    "            n_flatten = n_flatten.shape[1]\n",
    "        \"\"\"\n",
    "        n_flatten = 960\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU()) #n_flatten=960\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_basic_res_block(self, in_channel, out_channel):\n",
    "        return nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def feat_extract(self, observations: th.Tensor) -> th.Tensor:\n",
    "        #observations = observations.to(self.device)\n",
    "        observations = observations.float()\n",
    "        observations /= 255\n",
    "\n",
    "        conv_out = observations\n",
    "        for i in range(4):\n",
    "            #print(\"\", i)\n",
    "            #print(\" 1. conv_out.is_cuda() \", conv_out.is_cuda)\n",
    "            #print(\"     conv block weight\", self.conv_blocks[i].weight.is_cuda)\n",
    "            conv_out = self.conv_blocks[i](conv_out)\n",
    "            #print(\" 2. conv_out.is_cuda() \", conv_out.is_cuda)\n",
    "            conv_out = self.pools[i](conv_out)\n",
    "\n",
    "            block_input = conv_out\n",
    "            conv_out = self.resblocks_1[i](conv_out)\n",
    "            conv_out += block_input\n",
    "\n",
    "            block_input = conv_out\n",
    "            conv_out = self.resblocks_2[i](conv_out)\n",
    "            conv_out += block_input\n",
    "            #print(\" 3. conv_out.is_cuda() \", conv_out.is_cuda)\n",
    "\n",
    "        #print(\" before relu . conv_out.is_cuda() \", conv_out.is_cuda)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        #print(\" after relu . conv_out.is_cuda() \", conv_out.is_cuda)\n",
    "        conv_out = self.flatten(conv_out)\n",
    "        #print(\" after flatten . conv_out.is_cuda() \", conv_out.is_cuda)\n",
    "        return conv_out\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        conv_out = self.feat_extract(observations)\n",
    "        conv_out = self.linear(conv_out)\n",
    "\n",
    "        return conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "productive-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataSet(Dataset):\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accepted-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "        student,\n",
    "        env,\n",
    "        expert_dataset,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=1.0,\n",
    "        log_interval=100,\n",
    "        no_cuda=True,\n",
    "        seed=1,\n",
    "        test_batch_size=64,\n",
    "):\n",
    "    train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "    test_size = len(expert_dataset) - train_size\n",
    "\n",
    "    train_expert_dataset, test_expert_dataset = random_split(\n",
    "        expert_dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "    print(\"train_expert_dataset: \", len(train_expert_dataset))\n",
    "\n",
    "\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                latent_pi, _, _ = model._get_latent(data)\n",
    "                logits = model.action_net(latent_pi)\n",
    "                action_prediction = logits\n",
    "                target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(data)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    latent_pi, _, _ = model._get_latent(data)\n",
    "                    logits = model.action_net(latent_pi)\n",
    "                    action_prediction = logits\n",
    "                    target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    student.policy = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attended-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expert_data(env, num_interactions=1000):\n",
    "\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in tqdm(range(num_interactions)):\n",
    "        expert_observations.append(obs)\n",
    "\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        #print(info)\n",
    "        action = info[\"action_taken\"]\n",
    "        expert_actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    \n",
    "    expert_observations = np.array(expert_observations)\n",
    "    expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    expert_actions = np.array(expert_actions)\n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", expert_actions.shape)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        \"expert_data\",\n",
    "        expert_actions=expert_actions,\n",
    "        expert_observations=expert_observations,\n",
    "    )\n",
    "    return expert_observations, expert_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "distinct-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_perf_random_agent(env, num_trials=1):\n",
    "\n",
    "    obs = env.reset()\n",
    "    #env.render()\n",
    "    num_epi = 0\n",
    "    all_rewards = []\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(0, num_trials)):\n",
    "\n",
    "        done = False\n",
    "        total_r = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            #env.render()\n",
    "            total_r+=reward\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "                all_rewards.append(total_r)\n",
    "                total_r=0\n",
    "                num_epi +=1\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    return np.sum(all_rewards), np.std(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "endangered-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /home/ubuntu/ScenicGFootBall/rl_training\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Current Directory:\", cwd)\n",
    "rewards = \"scoring\"\n",
    "target_scenario_name = f\"{cwd}/pretrain/run_to_score.scenic\"\n",
    "\n",
    "save_dir = f\"{cwd}/pretrain_saved_models\"\n",
    "logdir = f\"{cwd}/tboard/dev\"\n",
    "tracedir = f\"{cwd}/game_trace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "typical-anaheim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#create target environment\n",
    "gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": rewards,\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"real_time\": False,\n",
    "        \"action_set\": \"default\"\n",
    "    }\n",
    "\n",
    "from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "scenario = buildScenario(target_scenario_name)\n",
    "target_env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facial-thousand",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ff6c08afb0fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdatagen_scenario_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{cwd}/pretrain/run_to_score_with_behave.scenic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdatagen_scenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildScenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatagen_scenario_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscenic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfootball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGFScenicEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/simulators/gfootball/utilities/scenic_helper.py\u001b[0m in \u001b[0;36mbuildScenario\u001b[0;34m(scenic_file, param, model, scenario)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuildScenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenic_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowInternalBacktrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     scenario = errors.callBeginningScenicTrace(\n\u001b[0m\u001b[1;32m     28\u001b[0m         lambda: translator.scenarioFromFile(scenic_file,\n\u001b[1;32m     29\u001b[0m                                             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/core/errors.py\u001b[0m in \u001b[0;36mcallBeginningScenicTrace\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mrunning\u001b[0m \u001b[0mScenic\u001b[0m \u001b[0mprograms\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msaveErrorLocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/simulators/gfootball/utilities/scenic_helper.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowInternalBacktrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     scenario = errors.callBeginningScenicTrace(\n\u001b[0;32m---> 28\u001b[0;31m         lambda: translator.scenarioFromFile(scenic_file,\n\u001b[0m\u001b[1;32m     29\u001b[0m                                             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/syntax/translator.py\u001b[0m in \u001b[0;36mscenarioFromFile\u001b[0;34m(path, params, model, scenario, cacheImports)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \t\treturn scenarioFromStream(stream, params=params, model=model, scenario=scenario,\n\u001b[0m\u001b[1;32m    106\u001b[0m \t\t\t\t\t\t\t\t  filename=fullpath, path=path, cacheImports=cacheImports)\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/syntax/translator.py\u001b[0m in \u001b[0;36mscenarioFromStream\u001b[0;34m(stream, params, model, scenario, filename, path, cacheImports)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtopLevelNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                         \u001b[0mcompileStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcacheImports\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/syntax/translator.py\u001b[0m in \u001b[0;36mcompileStream\u001b[0;34m(stream, namespace, params, model, filename)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# subsequent tokens are transformed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartitionByImports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mveneer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mnewSourceBlocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scenic/syntax/veneer.py\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(paramOverrides, modelOverride, filename)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevaluatingRequirement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevaluatingGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mcurrentSimulation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mnewScenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicScenario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# placeholder scenario for top-level code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mscenarioStack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewScenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#generate expert data\n",
    "\n",
    "#create data_gen_environment\n",
    "gf_env_settings = {\n",
    "    \"stacked\": True,\n",
    "    \"rewards\": 'scoring',\n",
    "    \"representation\": 'extracted',\n",
    "    \"players\": [f\"agent:left_players=1\"],\n",
    "    \"real_time\": False,\n",
    "    \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "}\n",
    "\n",
    "datagen_scenario_file = f\"{cwd}/pretrain/run_to_score_with_behave.scenic\"\n",
    "datagen_scenario = buildScenario(datagen_scenario_file)\n",
    "from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "\n",
    "\n",
    "datagen_env = GFScenicEnv(initial_scenario=datagen_scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True)\n",
    "print(\"Mean Reward and STD of Scenic Behavior Agent\", mean_perf_random_agent(datagen_env, num_trials=5))\n",
    "\n",
    "expert_observations, expert_actions = generate_expert_data(datagen_env, num_interactions=20000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-relief",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
