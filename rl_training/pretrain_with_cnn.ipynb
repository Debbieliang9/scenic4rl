{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "breathing-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic.simulators.gfootball import rl_interface\n",
    "from stable_baselines3 import PPO\n",
    "from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "import pretrain_template\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import os\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.preprocessing import is_image_space\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from torch import nn\n",
    "import torch as th\n",
    "import torch\n",
    "import os\n",
    "from pretrain.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "intimate-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "        student,\n",
    "        env,\n",
    "        expert_dataset,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=1.0,\n",
    "        log_interval=100,\n",
    "        no_cuda=True,\n",
    "        seed=1,\n",
    "        test_batch_size=64,\n",
    "):\n",
    "    train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "    test_size = len(expert_dataset) - train_size\n",
    "\n",
    "    train_expert_dataset, test_expert_dataset = random_split(\n",
    "        expert_dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "    print(\"train_expert_dataset: \", len(train_expert_dataset))\n",
    "\n",
    "\n",
    "    use_cuda = th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                latent_pi, _, _ = model._get_latent(data)\n",
    "                logits = model.action_net(latent_pi)\n",
    "                action_prediction = logits\n",
    "                target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(data)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    latent_pi, _, _ = model._get_latent(data)\n",
    "                    logits = model.action_net(latent_pi)\n",
    "                    action_prediction = logits\n",
    "                    target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "        return test_loss\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    #1.\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    \n",
    "    cons_lim = 2\n",
    "    cons=0\n",
    "    eps = 1e-5\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        \n",
    "        if test_loss < eps:\n",
    "            cons+=1 \n",
    "            if cons==cons_lim:\n",
    "                break\n",
    "        else:\n",
    "            cons=0\n",
    "            \n",
    "        #scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    print(f\"Trained for {epoch} epochs. Test Loss: {test_loss}\")\n",
    "    student.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "super-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_perf_agent(agent, env, num_trials=5):\n",
    "    \n",
    "    #env.render()\n",
    "    num_epi = 0\n",
    "    all_rewards = []\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(0, num_trials)):\n",
    "\n",
    "        done = False\n",
    "        total_r = 0\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action = agent.predict(obs, deterministic=True)[0]\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            #env.render()\n",
    "            total_r+=reward\n",
    "            if done:\n",
    "                all_rewards.append(total_r)\n",
    "                num_epi +=1 \n",
    "                \n",
    "    all_rewards = np.array(all_rewards)\n",
    "    return np.mean(all_rewards), np.std(all_rewards), all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "adjacent-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /home/ubuntu/ScenicGFootBall/rl_training\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Current Directory:\", cwd)\n",
    "rewards = \"scoring\"\n",
    "#target_scenario_name = f\"{cwd}/pretrain/run_to_score.scenic\"\n",
    "target_scenario_name = f\"{cwd}/pretrain/pass_n_shoot.scenic\"\n",
    "\n",
    "save_dir = f\"{cwd}/pretrain/saved_models_hp\"\n",
    "logdir = f\"{cwd}/tboard/dev/pretrain\"\n",
    "tracedir = f\"{cwd}/game_trace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "identical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create target environment\n",
    "gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": rewards,\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"real_time\": False,\n",
    "        \"action_set\": \"default\"\n",
    "    }\n",
    "\n",
    "from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "scenario = buildScenario(target_scenario_name)\n",
    "target_env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "stupid-banner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data obs: (2500, 16, 72, 96), actions: (2500,)\n"
     ]
    }
   ],
   "source": [
    "#Load Expert Data\n",
    "num_interactions = 2500\n",
    "saved_exp_data = f\"pretrain/expert_data/pass_n_shoot_{num_interactions}\"\n",
    "loaded_data = np.load(f\"{saved_exp_data}.npz\")\n",
    "expert_observations = loaded_data[\"expert_observations\"]\n",
    "expert_actions = loaded_data[\"expert_actions\"]\n",
    "expert_rewards = loaded_data[\"expert_rewards\"]\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "print(f\"Loaded data obs: {expert_observations.shape}, actions: {expert_actions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fuzzy-gates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert data mean rewards(std): 0.546875(0.4977978850648122)\n",
      "Total 64 Trajectories of mean length: 39.0625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expert data mean rewards(std): {np.mean(expert_rewards)}({np.std(expert_rewards)})\")\n",
    "print(f\"Total {expert_rewards.shape[0]} Trajectories of mean length: {expert_actions.shape[0]/expert_rewards.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bridal-reverse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "test_expert_dataset:  500\n",
      "train_expert_dataset:  2000\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 2.944442\n",
      "Test set: Average loss: 0.0027\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 1.288508\n",
      "Test set: Average loss: 0.0019\n",
      "Train Epoch: 3 [0/2000 (0%)]\tLoss: 0.718689\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 4 [0/2000 (0%)]\tLoss: 0.268336\n",
      "Test set: Average loss: 0.0006\n",
      "Train Epoch: 5 [0/2000 (0%)]\tLoss: 0.107026\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 6 [0/2000 (0%)]\tLoss: 0.058060\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 7 [0/2000 (0%)]\tLoss: 0.059523\n",
      "Test set: Average loss: 0.0002\n",
      "Train Epoch: 8 [0/2000 (0%)]\tLoss: 0.047977\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 9 [0/2000 (0%)]\tLoss: 0.049427\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 10 [0/2000 (0%)]\tLoss: 0.023883\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 11 [0/2000 (0%)]\tLoss: 0.035720\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 12 [0/2000 (0%)]\tLoss: 0.036797\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 13 [0/2000 (0%)]\tLoss: 0.052019\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 14 [0/2000 (0%)]\tLoss: 0.051064\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 15 [0/2000 (0%)]\tLoss: 0.073790\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 16 [0/2000 (0%)]\tLoss: 0.000494\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 17 [0/2000 (0%)]\tLoss: 0.003187\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 18 [0/2000 (0%)]\tLoss: 0.089681\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 19 [0/2000 (0%)]\tLoss: 0.000481\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 20 [0/2000 (0%)]\tLoss: 0.005819\n",
      "Test set: Average loss: 0.0002\n",
      "Train Epoch: 21 [0/2000 (0%)]\tLoss: 0.117844\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 22 [0/2000 (0%)]\tLoss: 0.006747\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 23 [0/2000 (0%)]\tLoss: 0.101946\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 24 [0/2000 (0%)]\tLoss: 0.050958\n",
      "Test set: Average loss: 0.0002\n",
      "Train Epoch: 25 [0/2000 (0%)]\tLoss: 0.030245\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 26 [0/2000 (0%)]\tLoss: 0.026667\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 27 [0/2000 (0%)]\tLoss: 0.003853\n",
      "Test set: Average loss: 0.0002\n",
      "Train Epoch: 28 [0/2000 (0%)]\tLoss: 0.001833\n",
      "Test set: Average loss: 0.0011\n",
      "Train Epoch: 29 [0/2000 (0%)]\tLoss: 0.062151\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 30 [0/2000 (0%)]\tLoss: 0.026539\n",
      "Test set: Average loss: 0.0002\n",
      "Train Epoch: 31 [0/2000 (0%)]\tLoss: 0.004672\n",
      "Test set: Average loss: 0.0004\n",
      "Train Epoch: 32 [0/2000 (0%)]\tLoss: 0.000572\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 33 [0/2000 (0%)]\tLoss: 0.019205\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 34 [0/2000 (0%)]\tLoss: 0.000557\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 35 [0/2000 (0%)]\tLoss: 0.009564\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 36 [0/2000 (0%)]\tLoss: 0.002662\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 37 [0/2000 (0%)]\tLoss: 0.010087\n",
      "Test set: Average loss: 0.0010\n",
      "Train Epoch: 38 [0/2000 (0%)]\tLoss: 0.018487\n",
      "Test set: Average loss: 0.0014\n",
      "Train Epoch: 39 [0/2000 (0%)]\tLoss: 0.011351\n",
      "Test set: Average loss: 0.0006\n",
      "Train Epoch: 40 [0/2000 (0%)]\tLoss: 0.076405\n",
      "Test set: Average loss: 0.0006\n",
      "Train Epoch: 41 [0/2000 (0%)]\tLoss: 0.002896\n",
      "Test set: Average loss: 0.0010\n",
      "Train Epoch: 42 [0/2000 (0%)]\tLoss: 0.015542\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 43 [0/2000 (0%)]\tLoss: 0.002384\n",
      "Test set: Average loss: 0.0006\n",
      "Train Epoch: 44 [0/2000 (0%)]\tLoss: 0.006347\n",
      "Test set: Average loss: 0.0008\n",
      "Train Epoch: 45 [0/2000 (0%)]\tLoss: 0.009283\n",
      "Test set: Average loss: 0.0006\n",
      "Train Epoch: 46 [0/2000 (0%)]\tLoss: 0.044936\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 47 [0/2000 (0%)]\tLoss: 0.003304\n",
      "Test set: Average loss: 0.0003\n",
      "Train Epoch: 48 [0/2000 (0%)]\tLoss: 0.058291\n",
      "Test set: Average loss: 0.0008\n",
      "Train Epoch: 49 [0/2000 (0%)]\tLoss: 0.010145\n",
      "Test set: Average loss: 0.0001\n",
      "Train Epoch: 50 [0/2000 (0%)]\tLoss: 0.009297\n",
      "Test set: Average loss: 0.0001\n",
      "Trained for 50 epochs. Test Loss: 6.939958257135004e-05\n"
     ]
    }
   ],
   "source": [
    "ppo_agent = PPO(\"CnnPolicy\", target_env, verbose=1)\n",
    "#ppo_agent.policy\n",
    "\n",
    "n_epochs = 50\n",
    "pretrain_agent(\n",
    "    student=ppo_agent,\n",
    "    env=target_env,\n",
    "    expert_dataset=expert_dataset,\n",
    "    epochs=n_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "pending-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, 0.5, array([0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 0.]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_performance_pretrained = mean_perf_agent(agent=ppo_agent, env=target_env, num_trials=20)\n",
    "print(mean_performance_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "passive-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent.save(f\"cnn_adam_pass_n_shoot_{num_interactions}_{n_epochs}\")\n",
    "del ppo_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "starting-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_agent = PPO.load(f\"cnn_adam_pass_n_shoot_{num_interactions}_{n_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "sound-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.45, 0.49749371855331004, array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_performance_pretrained = mean_perf_agent(agent=loaded_agent, env=target_env, num_trials=100)\n",
    "print(mean_performance_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-peace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-coaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-hampton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
