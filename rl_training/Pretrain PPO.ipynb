{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "revised-mainland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:52.922082Z",
     "start_time": "2021-03-25T23:48:52.781131Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utility-typing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.236261Z",
     "start_time": "2021-03-25T23:48:52.924496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "potential-record",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.748986Z",
     "start_time": "2021-03-25T23:48:53.238458Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "upset-collect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.757473Z",
     "start_time": "2021-03-25T23:48:53.750772Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bright-harvey",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.760857Z",
     "start_time": "2021-03-25T23:48:53.758866Z"
    }
   },
   "outputs": [],
   "source": [
    "num_interactions = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "funky-mining",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.798212Z",
     "start_time": "2021-03-25T23:48:53.762343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 56794.91it/s]\n"
     ]
    }
   ],
   "source": [
    "expert_observations = []\n",
    "expert_actions = []\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for i in tqdm(range(num_interactions)):\n",
    "    action = env.action_space.sample()\n",
    "    expert_observations.append(obs)\n",
    "    expert_actions.append(action) \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "expert_observations = np.array(expert_observations)\n",
    "expert_actions = np.array(expert_actions)\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"expert_data\",\n",
    "    expert_actions=expert_actions,\n",
    "    expert_observations=expert_observations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bigger-findings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.805956Z",
     "start_time": "2021-03-25T23:48:53.802373Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "another-button",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.812602Z",
     "start_time": "2021-03-25T23:48:53.809375Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bound-narrative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.817299Z",
     "start_time": "2021-03-25T23:48:53.814426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_expert_dataset:  200\n",
      "train_expert_dataset:  800\n"
     ]
    }
   ],
   "source": [
    "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "print(\"train_expert_dataset: \", len(train_expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "approximate-proceeding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.831529Z",
     "start_time": "2021-03-25T23:48:53.819505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "ppo_student = PPO('MlpPolicy', env_id, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "present-earth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:48:53.847108Z",
     "start_time": "2021-03-25T23:48:53.833316Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "      criterion = nn.MSELoss()\n",
    "    else:\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "              # A2C/PPO policy outputs actions, values, log_prob\n",
    "              # SAC/TD3 policy outputs actions only\n",
    "              if isinstance(student, (A2C, PPO)):\n",
    "                action, _, _ = model(data)\n",
    "              else:\n",
    "                # SAC/TD3:\n",
    "                action = model(data)\n",
    "              action_prediction = action.double()\n",
    "            else:\n",
    "              # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "              latent_pi, _, _ = model._get_latent(data)\n",
    "              logits = model.action_net(latent_pi)\n",
    "              action_prediction = logits\n",
    "              target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                  # A2C/PPO policy outputs actions, values, log_prob\n",
    "                  # SAC/TD3 policy outputs actions only\n",
    "                  if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                  else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                  action_prediction = action.double()\n",
    "                else:\n",
    "                  # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                  latent_pi, _, _ = model._get_latent(data)\n",
    "                  logits = model.action_net(latent_pi)\n",
    "                  action_prediction = logits\n",
    "                  target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    ppo_student.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "superior-murray",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:50:44.927253Z",
     "start_time": "2021-03-25T23:50:44.879176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward = 10.6 +/- 4.841487374764082\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_student, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "found-ambassador",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:50:46.190483Z",
     "start_time": "2021-03-25T23:50:45.992123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 0.695015\n",
      "Test set: Average loss: 0.0035\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 0.709005\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.698522\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 0.686600\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 0.688374\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 0.686452\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 0.707751\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 0.693284\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 0.691553\n",
      "Test set: Average loss: 0.0034\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 0.703895\n",
      "Test set: Average loss: 0.0034\n"
     ]
    }
   ],
   "source": [
    "pretrain_agent(\n",
    "    ppo_student,\n",
    "    epochs=10,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=1000,\n",
    ")\n",
    "ppo_student.save(\"ppo_student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "organizational-baseline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T23:50:49.961814Z",
     "start_time": "2021-03-25T23:50:49.918186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward = 10.8 +/- 3.762977544445356\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_student, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "musical-isaac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T01:34:57.297087Z",
     "start_time": "2021-03-26T01:34:57.292761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4, 5) [[[[  0   1   2   3   4]\n",
      "   [  5   6   7   8   9]\n",
      "   [ 10  11  12  13  14]\n",
      "   [ 15  16  17  18  19]]\n",
      "\n",
      "  [[ 20  21  22  23  24]\n",
      "   [ 25  26  27  28  29]\n",
      "   [ 30  31  32  33  34]\n",
      "   [ 35  36  37  38  39]]\n",
      "\n",
      "  [[ 40  41  42  43  44]\n",
      "   [ 45  46  47  48  49]\n",
      "   [ 50  51  52  53  54]\n",
      "   [ 55  56  57  58  59]]]\n",
      "\n",
      "\n",
      " [[[ 60  61  62  63  64]\n",
      "   [ 65  66  67  68  69]\n",
      "   [ 70  71  72  73  74]\n",
      "   [ 75  76  77  78  79]]\n",
      "\n",
      "  [[ 80  81  82  83  84]\n",
      "   [ 85  86  87  88  89]\n",
      "   [ 90  91  92  93  94]\n",
      "   [ 95  96  97  98  99]]\n",
      "\n",
      "  [[100 101 102 103 104]\n",
      "   [105 106 107 108 109]\n",
      "   [110 111 112 113 114]\n",
      "   [115 116 117 118 119]]]]\n",
      "(2, 4, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = np.arange(2*3*4*5).reshape(2, 3, 4, 5)\n",
    "print(test.shape,test)\n",
    "\n",
    "#test2 = np.moveaxis(test, [0,1,2,3], [0,2,3,1])\n",
    "\n",
    "test2 = np.moveaxis(test, [1], [3])\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "republican-fancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T01:37:05.471896Z",
     "start_time": "2021-03-26T01:37:05.468608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 21 22 23 24]\n",
      " [25 26 27 28 29]\n",
      " [30 31 32 33 34]\n",
      " [35 36 37 38 39]]\n"
     ]
    }
   ],
   "source": [
    "print(test2[0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-implementation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
