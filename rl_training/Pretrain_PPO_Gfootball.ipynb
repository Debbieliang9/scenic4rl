{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-yield",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:19.035538Z",
     "start_time": "2021-03-30T06:56:18.092841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azadsalam/codebase/scenic/src/scenic/core/errors.py:157: UserWarning: unable to install sys.excepthook to format Scenic backtraces\n",
      "  warnings.warn('unable to install sys.excepthook to format Scenic backtraces')\n"
     ]
    }
   ],
   "source": [
    "from scenic.simulators.gfootball import rl_interface\n",
    "from stable_baselines3 import PPO\n",
    "from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "import pretrain_template\n",
    "from gfootball_impala_cnn import GfootballImpalaCNN\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-majority",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:19.179892Z",
     "start_time": "2021-03-30T06:56:19.175890Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExpertDataSet(Dataset):\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moved-verification",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:20.021823Z",
     "start_time": "2021-03-30T06:56:20.005654Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "        student,\n",
    "        env,\n",
    "        expert_dataset,\n",
    "        batch_size=64,\n",
    "        epochs=10,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=1.0,\n",
    "        log_interval=100,\n",
    "        no_cuda=True,\n",
    "        seed=1,\n",
    "        test_batch_size=64,\n",
    "):\n",
    "    train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "    test_size = len(expert_dataset) - train_size\n",
    "\n",
    "    train_expert_dataset, test_expert_dataset = random_split(\n",
    "        expert_dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "    print(\"train_expert_dataset: \", len(train_expert_dataset))\n",
    "\n",
    "\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                latent_pi, _, _ = model._get_latent(data)\n",
    "                logits = model.action_net(latent_pi)\n",
    "                action_prediction = logits\n",
    "                target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(data)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    latent_pi, _, _ = model._get_latent(data)\n",
    "                    logits = model.action_net(latent_pi)\n",
    "                    action_prediction = logits\n",
    "                    target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    student.policy = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "driving-heritage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:21.269068Z",
     "start_time": "2021-03-30T06:56:21.265747Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "rewards = \"scoring\"\n",
    "target_scenario_name = f\"{cwd}/exp_0_4/academy_run_to_score.scenic\"\n",
    "\n",
    "save_dir = f\"{cwd}/saved_models\"\n",
    "logdir = f\"{cwd}/tboard/dev\"\n",
    "tracedir = f\"{cwd}/game_trace\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "every-ecology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:33.176989Z",
     "start_time": "2021-03-30T06:56:33.032389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.9.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#Create Environment\n",
    "\n",
    "gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": rewards,\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"real_time\": False,\n",
    "        \"action_set\": \"default\"\n",
    "    }\n",
    "\n",
    "from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "scenario = buildScenario(target_scenario_name)\n",
    "target_env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-burke",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:35.435171Z",
     "start_time": "2021-03-30T06:56:35.430571Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_reward_random_agent(env, num_trials=1):\n",
    "\n",
    "    obs = env.reset()\n",
    "    #env.render()\n",
    "    num_epi = 0\n",
    "    total_r = 0\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(0, num_trials)):\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            #env.render()\n",
    "            total_r+=reward\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "                num_epi +=1\n",
    "\n",
    "    return total_r/num_epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "labeled-longer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:56:39.219926Z",
     "start_time": "2021-03-30T06:56:36.149822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward of Scenic Behavior Agent 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate expert data\n",
    "\n",
    "gf_env_settings = {\n",
    "    \"stacked\": True,\n",
    "    \"rewards\": 'scoring',\n",
    "    \"representation\": 'extracted',\n",
    "    \"players\": [f\"agent:left_players=1\"],\n",
    "    \"real_time\": False,\n",
    "    \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "}\n",
    "\n",
    "datagen_scenario_file = f\"{cwd}/pretrain/run_to_score_with_behave.scenic\"\n",
    "datagen_scenario = buildScenario(datagen_scenario_file)\n",
    "from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "\n",
    "datagen_env = GFScenicEnv(initial_scenario=datagen_scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True)\n",
    "print(\"Mean Reward of Scenic Behavior Agent\", mean_reward_random_agent(datagen_env, num_trials=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "favorite-wallet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:57:03.210530Z",
     "start_time": "2021-03-30T06:56:56.500355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 165.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert observation shape:  (1000, 16, 72, 96)\n",
      "Expert actions shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_expert_data(env, num_interactions=1000):\n",
    "\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in tqdm(range(num_interactions)):\n",
    "        expert_observations.append(obs)\n",
    "\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        #print(info)\n",
    "        action = info[\"action_taken\"]\n",
    "        expert_actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    \n",
    "    expert_observations = np.array(expert_observations)\n",
    "    expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    expert_actions = np.array(expert_actions)\n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", expert_actions.shape)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        \"expert_data\",\n",
    "        expert_actions=expert_actions,\n",
    "        expert_observations=expert_observations,\n",
    "    )\n",
    "    return expert_observations, expert_actions\n",
    "\n",
    "expert_observations, expert_actions = generate_expert_data(datagen_env, num_interactions=1000)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-python",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:43:39.327475Z",
     "start_time": "2021-03-30T06:43:32.613828Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate random data for now as proxy to expert data\n",
    "\n",
    "\n",
    "def gen_dummy_expert_data(num_interactions=1000):\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in tqdm(range(num_interactions)):\n",
    "        action = env.action_space.sample()\n",
    "        expert_observations.append(obs)\n",
    "        expert_actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "\n",
    "    expert_observations = np.array(expert_observations)\n",
    "    print(\"expert collected obs shape\", expert_observations.shape)\n",
    "    expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    print(\"expert obs shape updated: \", expert_observations.shape)\n",
    "    expert_actions = np.array(expert_actions)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        \"dummy_expert_data\",\n",
    "        expert_actions=expert_actions,\n",
    "        expert_observations=expert_observations,\n",
    "    )\n",
    "\n",
    "    return expert_observations, expert_actions\n",
    "\n",
    "dummy_expert_observations, dummy_expert_actions = gen_dummy_expert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-preserve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T08:21:04.469234Z",
     "start_time": "2021-03-28T08:21:04.465295Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occasional-marking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:57:27.432065Z",
     "start_time": "2021-03-30T06:57:27.404839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scoring Parameters\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "device:  cpu\n",
      "env (from model) observation space:  Box(0, 255, (16, 72, 96), uint8)\n"
     ]
    }
   ],
   "source": [
    "#get the PPO object with required parameters\n",
    "model, parameter_dict = pretrain_template.get_model_and_params(\n",
    "    env=target_env, ALGO=PPO, features_extractor_class = GfootballImpalaCNN, scenario_name=target_scenario_name,\n",
    "    logdir=logdir, override_params={}, rewards=rewards)\n",
    "\n",
    "print(\"env (from model) observation space: \", model.get_env().observation_space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecological-arrest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:57:33.732369Z",
     "start_time": "2021-03-30T06:57:33.727978Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_data = np.load(\"expert_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "likely-beatles",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:57:34.826550Z",
     "start_time": "2021-03-30T06:57:34.617503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data obs: (1000, 16, 72, 96), actions: (1000,)\n"
     ]
    }
   ],
   "source": [
    "expert_observations = loaded_data[\"expert_observations\"]\n",
    "expert_actions = loaded_data[\"expert_actions\"]\n",
    "print(f\"Loaded data obs: {expert_observations.shape}, actions: {expert_actions.shape}\")\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fifteen-warren",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:58:56.489751Z",
     "start_time": "2021-03-30T06:57:41.882119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_expert_dataset:  200\n",
      "train_expert_dataset:  800\n",
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 2.944235\n",
      "Test set: Average loss: 0.0061\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 1.163416\n",
      "Test set: Average loss: 0.0040\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 0.894178\n",
      "Test set: Average loss: 0.0043\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 1.104846\n",
      "Test set: Average loss: 0.0057\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 1.068910\n",
      "Test set: Average loss: 0.0057\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 1.111425\n",
      "Test set: Average loss: 0.0079\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 1.156394\n",
      "Test set: Average loss: 0.0080\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 1.162453\n",
      "Test set: Average loss: 0.0058\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 1.271063\n",
      "Test set: Average loss: 0.0062\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 1.195923\n",
      "Test set: Average loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "pretrain_agent(\n",
    "    student=model,\n",
    "    env=target_env,\n",
    "    expert_dataset=expert_dataset,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cathedral-spine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:59:18.315830Z",
     "start_time": "2021-03-30T06:59:07.499361Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_model_performance(env, model, num_trials=1):\n",
    "\n",
    "    obs = env.reset()\n",
    "    #env.render()\n",
    "    num_epi = 0\n",
    "    total_r = 0\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(0, num_trials)):\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)[0]\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            #env.render()\n",
    "            total_r+=reward\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "                num_epi +=1\n",
    "\n",
    "    return total_r/num_epi\n",
    "\n",
    "print(test_model_performance(target_env, model, num_trials=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pending-portal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T07:13:06.405964Z",
     "start_time": "2021-03-30T06:59:29.516790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /Users/azadsalam/codebase/scenic/rl_training/tboard/dev/HM_23_59__DM_29_3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.6     |\n",
      "|    ep_rew_mean     | 0.667    |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 80           |\n",
      "|    ep_rew_mean          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 264          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018253119 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.115        |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 5.65e-05     |\n",
      "|    learning_rate        | 0.000119     |\n",
      "|    loss                 | 0.0194       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 0.0497       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=0.20 +/- 0.40\n",
      "Episode length: 96.20 +/- 10.57\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 96.2         |\n",
      "|    mean_reward          | 0.2          |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 78.9         |\n",
      "|    ep_rew_mean          | 0.649        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 576          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040503265 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.115        |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 3.64e-05     |\n",
      "|    learning_rate        | 0.000119     |\n",
      "|    loss                 | 0.016        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0033      |\n",
      "|    value_loss           | 0.0447       |\n",
      "------------------------------------------\n",
      "\n",
      "Eval Mean Rewards: 0.4000 Episodes: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do training\n",
    "pretrain_template.train(model=model, parameters=parameter_dict,\n",
    "                            n_eval_episodes=5, total_training_timesteps=5000,\n",
    "                            eval_freq=5000,\n",
    "                            save_dir=save_dir, logdir=logdir, dump_info={\"rewards\": rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-excitement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
