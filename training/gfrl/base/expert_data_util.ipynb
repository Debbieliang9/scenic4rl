{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d3aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code is used to train BC imitator, or pretrained GAIL imitator\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import tempfile\n",
    "import os.path as osp\n",
    "import gym\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from baselines.gail import mlp_policy\n",
    "from baselines import bench\n",
    "from baselines import logger\n",
    "from baselines.common import set_global_seeds, tf_util as U\n",
    "from baselines.common.misc_util import boolean_flag\n",
    "\n",
    "#from baselines.gail.run_mujoco import runner\n",
    "from baselines.gail.dataset.mujoco_dset import Mujoco_Dset \n",
    "\n",
    "##from baselines.common.mpi_adam import MpiAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4140f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expert_data_with_rewards(scenario_file, num_interactions=1000, file_name=\"expert_data\", act_ndim=19):\n",
    "    #from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "    import numpy as np\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "    expert_rewards = []\n",
    "\n",
    "    gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": 'scoring',\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "    }\n",
    "\n",
    "    from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "    from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "    scenario = buildScenario(scenario_file)\n",
    "    env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True, constraints_checking=True)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    tr = 0\n",
    "    for i in tqdm(range(num_interactions)):\n",
    "        expert_observations.append(obs)\n",
    "\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        tr+=reward\n",
    "        # print(info)\n",
    "        action = info[\"action_taken\"]\n",
    "        expert_actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            expert_rewards.append(tr)\n",
    "            tr = 0\n",
    "\n",
    "    expert_observations = np.array(expert_observations)\n",
    "    #expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    acts = np.array(expert_actions)\n",
    "\n",
    "    acts_oh = np.zeros((acts.shape[0], act_ndim))\n",
    "    acts_oh[np.arange(acts.shape[0]), acts] = 1\n",
    "\n",
    "    expert_rewards = np.array(expert_rewards)\n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", acts_oh.shape)\n",
    "    print(\"Num Episode: \", expert_rewards.shape[0])\n",
    "    print(\"Mean Reward: \", expert_rewards.mean())\n",
    "\n",
    "    #np.savez(expert_file_name, obs=mb_obs, acs=mb_act_oh, num_epi = num_episodes, mean_reward = np.sum(mb_rewards)/num_episodes)\n",
    "    np.savez_compressed(\n",
    "        file_name,\n",
    "        acs=acts_oh,\n",
    "        obs=expert_observations,\n",
    "        num_epi = expert_rewards.shape[0],\n",
    "        mean_reward = expert_rewards.mean(),\n",
    "        rewards = expert_rewards\n",
    "    )\n",
    "    return expert_observations, acts_oh, expert_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63996680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 111.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert observation shape:  (1000, 72, 96, 16)\n",
      "Expert actions shape:  (1000, 19)\n",
      "Num Episode:  27\n",
      "Mean Reward:  0.6296296296296297\n"
     ]
    }
   ],
   "source": [
    "from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=1000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d36815ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "td = np.load(expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7810278a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acs', 'obs', 'num_epi', 'mean_reward', 'rewards']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(td.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45470961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f40d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0.6296296296296297 1000 (1000, 19)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732f422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:12<00:00, 75.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert observation shape:  (10000, 72, 96, 16)\n",
      "Expert actions shape:  (10000, 19)\n",
      "Num Episode:  243\n",
      "Mean Reward:  0.5390946502057613\n"
     ]
    }
   ],
   "source": [
    "from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=10000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd8c87c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 0.5390946502057613 10000 (10000, 19)\n"
     ]
    }
   ],
   "source": [
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa0883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2110274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:35<00:00, 52.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert observation shape:  (5000, 72, 96, 16)\n",
      "Expert actions shape:  (5000, 19)\n",
      "Num Episode:  116\n",
      "Mean Reward:  0.5517241379310345\n"
     ]
    }
   ],
   "source": [
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=5000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f20e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 0.5517241379310345 5000 (5000, 19)\n"
     ]
    }
   ],
   "source": [
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b27c1107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [14:47<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert observation shape:  (20000, 72, 96, 16)\n",
      "Expert actions shape:  (20000, 19)\n",
      "Num Episode:  505\n",
      "Mean Reward:  0.5089108910891089\n"
     ]
    }
   ],
   "source": [
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=20000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290f0bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 0.5089108910891089 20000 (20000, 19)\n"
     ]
    }
   ],
   "source": [
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5351dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
