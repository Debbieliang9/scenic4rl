{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a4419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfrl.base.run_my_ppo2 import create_single_scenic_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a762c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"../_scenarios/academy/3v1.scenic\"\n",
    "#env_single = create_single_scenic_environment(0, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cccc188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "vec_env = SubprocVecEnv([lambda _i=i: \\\n",
    "                        create_single_scenic_environment(_i,scenario) for i in\n",
    "                        range(16)], context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d06764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import multiprocessing\n",
    "#import tensorflow as tf\n",
    "\n",
    "ncpu = multiprocessing.cpu_count()\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        intra_op_parallelism_threads=ncpu,\n",
    "                        inter_op_parallelism_threads=ncpu)\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.Session(config=config).__enter__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9279f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b110809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(onsobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79843c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91eca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = \"gfootball_impala_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14886ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfrl.common.mybase import ppo2\n",
    "model = ppo2.learn(\n",
    "    network=network,\n",
    "    total_timesteps=0,\n",
    "    env=vec_env,\n",
    "    nsteps=512,\n",
    "    load_path=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6f919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2c3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from baselines.common.runners import AbstractEnvRunner\n",
    "\n",
    "class MyRunner(AbstractEnvRunner):\n",
    "    \"\"\"\n",
    "    We use this object to make a mini batch of experiences\n",
    "    __init__:\n",
    "    - Initialize the runner\n",
    "    run():\n",
    "    - Make a mini batch\n",
    "    \"\"\"\n",
    "    def __init__(self, *, env, model, nsteps, gamma, lam):\n",
    "        super().__init__(env=env, model=model, nsteps=nsteps)\n",
    "        # Lambda used in GAE (General Advantage Estimation)\n",
    "        self.lam = lam\n",
    "        # Discount rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def run(self):\n",
    "        # Here, we init the lists that will contain the mb of experiences\n",
    "        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones, mb_neglogpacs = [],[],[],[],[],[]\n",
    "        mb_states = self.states\n",
    "        epinfos = []\n",
    "        # For n in range number of steps\n",
    "        for _ in range(self.nsteps):\n",
    "            # Given observations, get action value and neglopacs\n",
    "            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n",
    "            actions, values, self.states, neglogpacs = self.model.step(self.obs, S=self.states, M=self.dones)\n",
    "            mb_obs.append(self.obs.copy())\n",
    "            mb_actions.append(actions)\n",
    "            mb_values.append(values)\n",
    "            mb_neglogpacs.append(neglogpacs)\n",
    "            mb_dones.append(self.dones)\n",
    "\n",
    "            # Take actions in env and look the results\n",
    "            # Infos contains a ton of useful informations\n",
    "            self.obs[:], rewards, self.dones, infos = self.env.step(actions)\n",
    "            for info in infos:\n",
    "                maybeepinfo = info.get('episode')\n",
    "                if maybeepinfo: epinfos.append(maybeepinfo)\n",
    "            mb_rewards.append(rewards)\n",
    "        #batch of steps to batch of rollouts\n",
    "        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n",
    "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
    "        mb_actions = np.asarray(mb_actions)\n",
    "        mb_values = np.asarray(mb_values, dtype=np.float32)\n",
    "        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n",
    "        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
    "        last_values = self.model.value(self.obs, S=self.states, M=self.dones)\n",
    "\n",
    "        # discount/bootstrap off value fn\n",
    "        mb_returns = np.zeros_like(mb_rewards)\n",
    "        mb_advs = np.zeros_like(mb_rewards)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(self.nsteps)):\n",
    "            if t == self.nsteps - 1:\n",
    "                nextnonterminal = 1.0 - self.dones\n",
    "                nextvalues = last_values\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - mb_dones[t+1]\n",
    "                nextvalues = mb_values[t+1]\n",
    "            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n",
    "            mb_advs[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n",
    "        mb_returns = mb_advs + mb_values\n",
    "        return (*map(sf01, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs)),\n",
    "            mb_states, epinfos)\n",
    "# obs, returns, masks, actions, values, neglogpacs, states = runner.run()\n",
    "def sf01(arr):\n",
    "    \"\"\"\n",
    "    swap and then flatten axes 0 and 1\n",
    "    \"\"\"\n",
    "    s = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7846f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = SubprocVecEnv([lambda _i=i: \\\n",
    "                        create_single_scenic_environment(_i,scenario) for i in\n",
    "                        range(16)], context=None)\n",
    "runner = MyRunner(env=vec_env, model=model, nsteps=512, gamma=0.95, lam=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a24681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2877c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7ac95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431667b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
